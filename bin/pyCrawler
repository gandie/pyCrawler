#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# Copyright (c) 2017 Lars Bergmann
#
# GNU GENERAL PUBLIC LICENSE
#    Version 3, 29 June 2007
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

'''
executable of crawler package, parsing arguments from command line
'''

# builtin
import argparse

# custom
import crawler.crawler as pyCrawler

if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument('url', help='This is the url to start crawling from')
    parser.add_argument(
        '-d',
        '--depth',
        help='How many generation of links to follow. Default 10',
        type=int,
        default=10
    )
    parser.add_argument(
        '-n',
        '--numworkers',
        help='Number of crawlers spawned. Default 25',
        type=int,
        default=25
    )
    parser.add_argument(
        '-r',
        '--release',
        help='Allow the crawler to leave host given by url',
        action='store_true',
        default=False
    )

    parser.add_argument(
        '-j',
        '--javascript',
        help='Parse custom Javascript found on page to find more urls',
        action='store_true',
        default=False
    )

    parser.add_argument(
        '-k',
        '--keyword',
        help='Give optional keyword to search for',
        default=None
    )

    args = parser.parse_args()

    pycrawler = pyCrawler.Crawler(
        starturl=args.url,
        depth=args.depth,
        numworkers=args.numworkers,
        release=args.release,
        keyword=args.keyword,
        javascript=args.javascript
    )
    pycrawler.run()
